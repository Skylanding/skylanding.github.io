{"config":{"lang":["en","zh"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Yu Li","text":"<p>George Washington University, Washington, D.C. 2025.9\u20132029.5(expected)      Ph.D. in Electrical and Computer Engineering (GPA:4.0/4.0)    </p> <p>Wuhan University, Hongyi Honor College, China 2021.9-2025.5      B.Eng. in Microelectronics Science and Technology (GPA: 3.87/4.0)    </p> <p></p> <p>I am currently a first year Ph.D. candidate at GWU supervised by Prof. Tian Lan and work with Prof.Zhengling Qi. </p> <p>Research Topics: Reinforcement Learning \u2022 Post-Training \u2022 Generative AI</p> <ul> <li>\ud83d\udcc4 CV</li> <li>\ud83e\uddea GitHub</li> <li>\ud83c\udf93 Google Scholar</li> <li>\ud83d\udcbc LinkedIn</li> </ul>"},{"location":"#news","title":"News","text":"<ul> <li>[02/2026] CRAFT-LORA is accepted to CVPR 2026 \ud83c\udf89. See you in Denver in June!</li> <li>[02/2026] ACDZero is accepted to ICCN@INFOCOM2026 \ud83c\udf89. See you in Tokyo in May!</li> <li>[01/2026] I passed my PhD qualifying exam in my first semester and am now a PhD candidate \ud83c\udf93.</li> <li>[01/2026] KG-SAM is accepted to ICASSP 2026 \ud83c\udf89.</li> <li>[11/2025] SoRA is accepted to AAAI 2026 \ud83c\udf89.</li> </ul>"},{"location":"#publications","title":"Publications","text":""},{"location":"#preprint-under-review","title":"Preprint / Under Review","text":"Unlocking Implicit Self-Reflection in Preference Optimization for LLM Alignment Aug. 2025 \u2013 Nov. 2025 \u00b7 Paper \u00b7 Code Preference optimization that leverages implicit self-reflection signals in pairwise data to improve LLM alignment. DPOSimPOPreference Learning Aligning LLMs with Finite State Machine Logic for Multi-turn Verilog Code Generation Sept. 2025 \u2013 Nov. 2025 Structured alignment to teach LLMs finite-state transition logic for multi-turn Verilog generation. RLVRCode GenerationVerilog FSM"},{"location":"#conferences","title":"Conferences","text":"CRAFT-LORA: Content-Style Personalization via Rank-Constrained Adaptation Yu Li, Yujun Cai, Chi Zhang    CVPR 2026 \u00b7 Paper Rank-constrained LoRA adaptation for content\u2013style personalization in image generation. Generative AIPersonalizationLoRA ACDZero: MCTS Agent for Mastering Automated Cyber Defense Yu Li*, Sizhe Tang*, Rongqian Chen, Fei Xu Yu, Guangyu Jiang, Mahdi Imani, Nathaniel D. Bastian, Tian Lan    (*:Equal contribution) ICCN@INFOCOM 2026 \u00b7 Paper \u00b7 Code Graph-embedding-guided MCTS planning for sample-efficient automated cyber defense. Cyber DefenseMCTSGNN KG-SAM: Injecting Anatomical Knowledge into Segment Anything Models via Conditional Random Fields Yu Li, Da Chang, Xi Xiao    ICASSP 2026 \u00b7 Paper \u00b7 Code Knowledge-guided SAM with anatomical priors and CRF refinement for robust medical image segmentation. Medical SegmentationSAMKnowledge GraphCRF Calibrating and Rotating: A Unified Framework for Weight Conditioning in PEFT      Da Chang, Peng Xue, Yu Li , Yongxiang Liu, Pengxiang Xu, Shixun Zhang    AAAI 2026 \u00b7 Paper \u00b7 Code A unified calibration+rotation weight-conditioning framework that improves PEFT performance and efficiency. LLMs PEFTWeight Conditioning Mixed Text Recognition with Efficient Parameter Fine-Tuning and Transformer Yu Li*, Chang Da*    (*:Equal contribution) ICONIP 2024 \u00b7 Paper \u00b7 Code TrOCR-based mixed-text OCR with efficient PEFT and an end-to-end evaluation pipeline. OCRLoRA"},{"location":"#journals","title":"Journals","text":"Dual branch SAM-Transformer Fusion Network for Accurate Breast Ultrasound Image Segmentation Y. Li, J. Huang et al.    Medical Physics, JCR Q1, 2025 \u00b7 Paper \u00b7 Code Dual-branch SAM\u2013Transformer fusion for accurate breast ultrasound image segmentation. Ultrasound SegmentationSAMTransformer SfMDiffusion: Self-supervised Monocular Depth Estimation in Endoscopy Based on Diffusion Models Y. Li, D. Chang et al.     International Journal of Computer Assisted Radiology and Surgery, JCR Q2, 2025 \u00b7 Paper \u00b7 Code Self-supervised monocular depth estimation for endoscopy using diffusion models with teacher-guided distillation. Depth EstimationDiffusion ModelDistillation"},{"location":"#experiences","title":"Experiences","text":"Mobile Intelligence Lab, George Washington University      Research Topic: Post-training, RL, Reasoning    Advisor: Prof. Tian Lan \u00b7 Aug. 2025 \u2013 Present Artificial General Intelligence Lab, Westlake University      Research Topic: Generative AI    Advisor: Prof. Chi Zhang \u00b7 Mar. 2025 \u2013 Jul. 2025 Cyber-Physical Systems Lab, UC Irvine      Research Topic: Multimodal Uncertainty Fusion    Advisor: Prof. Mohammad Al Faruque \u00b7 Jun. 2024 \u2013 Oct. 2024"},{"location":"#honors-and-awards","title":"Honors and Awards","text":"<ul> <li>Innova International Exchange Scholarship, Wuhan University, 2024</li> <li>Innova Excellence Scholarship (Top 3%), Wuhan University, 2023, 2024</li> <li>Academic Excellence Scholarship (Top 5%), Hongyi Honor College, 2022, 2023, 2024</li> <li>First-Class Scholarship (Top 5%), Wuhan University, 2022, 2023, 2024</li> <li>Patent: Energy-saving calculation method, CN116085952.</li> </ul>"},{"location":"#academic-services","title":"Academic Services","text":"<ul> <li>Conference Reviewer: ICML\u201926, CVPR\u201926, ICLR\u201926, AAAI\u201926, ICASSP\u201926</li> <li>Journal Reviewer: Neurocomputing, Frontiers in Oncology, IEEE Transactions on Networking, Frontiers in Medicine</li> </ul> <p>This website was stolen from my best friend CD.</p>"},{"location":"blog/","title":"Blog","text":"<p>Welcome to my blog. I use this space to share research notes, reproducibility tips, and personal reflections.</p>"},{"location":"blog/#posts","title":"Posts","text":"<ul> <li>Coming soon.</li> </ul>"},{"location":"blog/#topics","title":"Topics","text":"<ul> <li>Reinforcement Learning &amp; post-training</li> <li>Generative AI</li> <li>Research engineering notes (experiments, tooling, writing)</li> </ul>"},{"location":"zh/","title":"\u674e\u715c (Yu Li)","text":"<p>\u4e54\u6cbb\u00b7\u534e\u76db\u987f\u5927\u5b66 (George Washington University), \u534e\u76db\u987f\u7279\u533a. 2025.9-2029.5(\u9884\u8ba1)      \u7535\u5b50\u4e0e\u8ba1\u7b97\u673a\u5de5\u7a0b\u7cfb (ECE) \u535a\u58eb    </p> <p>\u6b66\u6c49\u5927\u5b66(Wuhan University) \u5f18\u6bc5\u5b66\u58022021\u7ea7      \u5fae\u7535\u5b50\u79d1\u5b66\u4e0e\u6280\u672f \u5de5\u5b66\u5b66\u58eb (GPA: 3.87/4.0)    </p> <p></p> <p>\u6211\u76ee\u524d\u662f\u4e54\u6cbb\u00b7\u534e\u76db\u987f\u5927\u5b66 \u4e00\u5e74\u7ea7\u535a\u58eb\u5019\u9009\u4eba\uff0c\u5bfc\u5e08\u662f Tian Lan \u6559\u6388\uff0c\u5e76\u4e0e Zhengling Qi \u6559\u6388\u5408\u4f5c\u3002</p> <p>\u7814\u7a76\u65b9\u5411: \u5f3a\u5316\u5b66\u4e60 \u2022 \u540e\u8bad\u7ec3\u7b56\u7565\u4f18\u5316 \u2022 \u751f\u6210\u5f0fAI</p> <ul> <li>\ud83d\udcc4 \u7b80\u5386</li> <li>\ud83e\uddea GitHub</li> <li>\ud83c\udf93 \u8c37\u6b4c\u5b66\u672f</li> <li>\ud83d\udcbc \u9886\u82f1</li> </ul>"},{"location":"zh/#news","title":"\u8fd1\u671f\u52a8\u6001 (News)","text":"<ul> <li>[02/2026] CRAFT-LORA \u88ab CVPR 2026 \u63a5\u6536 \ud83c\udf89. 6\u6708\u4e39\u4f5b\u89c1\uff01</li> <li>[02/2026] ACDZero \u88ab ICCN@INFOCOM 2026 \u63a5\u6536 \ud83c\udf89. 5\u6708\u4e1c\u4eac\u89c1!</li> <li>[01/2026] \u6211\u5728\u7b2c\u4e00\u5b66\u671f\u5c31\u901a\u8fc7\u4e86\u535a\u58eb\u8d44\u683c\u8003\u8bd5\uff01\u73b0\u5728\u5df2\u6210\u4e3a\u535a\u58eb\u5019\u9009\u4eba\uff08PhD Candidate\uff09\ud83c\udf93.</li> <li>[01/2026] KG-SAM \u88ab ICASSP 2026 \u63a5\u6536 \ud83c\udf89.</li> <li>[11/2025] SoRA \u88ab AAAI 2026 \u63a5\u6536 \ud83c\udf89.</li> </ul>"},{"location":"zh/#publications","title":"\u8bba\u6587\u53d1\u8868 (Publications)","text":""},{"location":"zh/#preprint-under-review","title":"\u9884\u5370\u672c / \u5728\u6295 (Preprint / Under Review)","text":"Unlocking Implicit Self-Reflection in Preference Optimization for LLM Alignment 2025.8 \u2013 2025.11 \u00b7 \u8bba\u6587 \u00b7 \u4ee3\u7801 \u4ece\u504f\u597d\u5bf9\u4e2d\u6316\u6398\u9690\u5f0f\u201c\u81ea\u6211\u53cd\u601d\u201d\u4fe1\u53f7\uff0c\u63d0\u5347\u504f\u597d\u4f18\u5316\u4e0e LLM \u5bf9\u9f50\u6548\u679c\u3002 \u76f4\u63a5\u504f\u597d\u4f18\u5316\u7b80\u5355\u504f\u597d\u4f18\u5316\u5f3a\u5316\u5b66\u4e60 Aligning LLMs with Finite State Machine Logic for Multi-turn Verilog Code Generation 2025.8 \u2013 2025.11 \u901a\u8fc7\u7ed3\u6784\u5316\u5bf9\u9f50\u8ba9 LLM \u5b66\u4e60\u6709\u9650\u72b6\u6001\u673a\u8f6c\u79fb\u903b\u8f91\uff0c\u5b9e\u73b0\u591a\u8f6e Verilog \u751f\u6210\u3002 RLVR\u4ee3\u7801\u751f\u6210Verilog \u72b6\u6001\u673a"},{"location":"zh/#conferences","title":"\u4f1a\u8bae\u8bba\u6587 (Conferences)","text":"CRAFT-LORA: Content-Style Personalization via Rank-Constrained Adaptation Yu Li, Yujun Cai, Chi Zhang    CVPR 2026 \u00b7 \u8bba\u6587 \u79e9\u7ea6\u675f LoRA \u9002\u914d\uff0c\u5b9e\u73b0\u5185\u5bb9-\u98ce\u683c\u89e3\u8026\u4e0e\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u3002 \u751f\u6210\u5f0fAI\u4e2a\u6027\u5316\u751f\u56feLoRA ACDZero: MCTS Agent for Mastering Automated Cyber Defense Yu Li*, Sizhe Tang*, Rongqian Chen, Fei Xu Yu, Guangyu Jiang, Mahdi Imani, Nathaniel D. Bastian, Tian Lan    (*:Equal contribution) ICCN@INFOCOM 2026 \u00b7 \u8bba\u6587 \u00b7 \u4ee3\u7801 \u7ed3\u5408\u56fe\u8868\u793a\u5b66\u4e60\u7684 MCTS \u89c4\u5212\u65b9\u6cd5\uff0c\u7528\u4e8e\u6837\u672c\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u7f51\u7edc\u9632\u5fa1\u3002 \u7f51\u7edc\u9632\u5fa1MCTSGNN KG-SAM: Injecting Anatomical Knowledge into Segment Anything Models via Conditional Random Fields Yu Li, Da Chang, Xi Xiao    ICASSP 2026 \u00b7 \u8bba\u6587 \u00b7 \u4ee3\u7801 \u878d\u5408\u77e5\u8bc6\u56fe\u8c31\u89e3\u5256\u5148\u9a8c\u4e0e CRF \u8fb9\u754c\u4f18\u5316\u7684\u77e5\u8bc6\u5f15\u5bfc SAM \u533b\u5b66\u5206\u5272\u6846\u67b6\u3002 \u533b\u5b66\u5206\u5272SAM\u77e5\u8bc6\u56fe\u8c31CRF Calibrating and Rotating: A Unified Framework for Weight Conditioning in PEFT      Da Chang, Peng Xue, Yu Li, Yongxiang Liu, Pengxiang Xu, Shixun Zhang    AAAI 2026 \u00b7 \u8bba\u6587 \u00b7 \u4ee3\u7801 \u7edf\u4e00\u201c\u6821\u51c6 + \u65cb\u8f6c\u201d\u7684\u6743\u91cd\u6761\u4ef6\u5316\u7b56\u7565\uff0c\u63d0\u5347 PEFT \u6027\u80fd\u4e0e\u8bad\u7ec3/\u63a8\u7406\u6548\u7387\u3002 LLMs PEFT\u6743\u91cd\u6761\u4ef6\u5316 Mixed Text Recognition with Efficient Parameter Fine-Tuning and Transformer Yu Li*, Chang Da*    (*:Equal contribution) ICONIP 2024 \u00b7 \u8bba\u6587 \u00b7 \u4ee3\u7801 \u57fa\u4e8e TrOCR + PEFT \u7684\u6df7\u5408\u6587\u672c\u8bc6\u522b\u65b9\u6cd5\u4e0e\u5b9e\u7528\u8bc4\u4f30\u6d41\u7a0b\u3002 OCRLoRA"},{"location":"zh/#journals","title":"\u671f\u520a\u8bba\u6587 (Journals)","text":"Dual branch SAM-Transformer Fusion Network for Accurate Breast Ultrasound Image Segmentation Y. Li, J. Huang et al.    Medical Physics, JCR Q1, 2025 \u00b7 \u8bba\u6587 \u00b7 \u4ee3\u7801 \u53cc\u5206\u652f SAM\u2013Transformer \u878d\u5408\u7f51\u7edc\uff0c\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u4e73\u817a\u8d85\u58f0\u5206\u5272\u3002 \u8d85\u58f0\u5206\u5272SAMTransformer SfMDiffusion: Self-supervised Monocular Depth Estimation in Endoscopy Based on Diffusion Models Y. Li, D. Chang et al.     International Journal of Computer Assisted Radiology and Surgery, JCR Q2, 2025 \u00b7 \u8bba\u6587 \u00b7 \u4ee3\u7801 \u9762\u5411\u5185\u7aa5\u955c\u573a\u666f\u7684\u81ea\u76d1\u7763\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\uff1a\u6269\u6563\u6a21\u578b\u7ed3\u5408\u6559\u5e08\u5f15\u5bfc\u84b8\u998f\u3002 \u6df1\u5ea6\u4f30\u8ba1\u6269\u6563\u6a21\u578b\u77e5\u8bc6\u84b8\u998f"},{"location":"zh/#experiences","title":"\u79d1\u7814\u7ecf\u5386 (Experiences)","text":"\u4e54\u6cbb\u00b7\u534e\u76db\u987f\u5927\u5b66, \u79fb\u52a8\u667a\u80fd\u5b9e\u9a8c\u5ba4      \u7814\u7a76\u8bfe\u9898: \u8bad\u7ec3\u540e\u4f18\u5316, \u5f3a\u5316\u5b66\u4e60, \u63a8\u7406    \u5bfc\u5e08: Prof. Tian Lan \u00b7 2025\u5e748\u6708 \u2013 \u81f3\u4eca \u897f\u6e56\u5927\u5b66, \u4eba\u5de5\u901a\u7528\u667a\u80fd\u5b9e\u9a8c\u5ba4      \u7814\u7a76\u8bfe\u9898: \u751f\u6210\u5f0fAI    \u5bfc\u5e08: Prof. Chi Zhang \u00b7 2025\u5e743\u6708 \u2013 2025\u5e747\u6708 \u52a0\u5dde\u5927\u5b66\u5c14\u6e7e\u5206\u6821, \u4fe1\u606f\u7269\u7406\u7cfb\u7edf\u5b9e\u9a8c\u5ba4      \u7814\u7a76\u8bfe\u9898: \u591a\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\u878d\u5408    \u5bfc\u5e08: Prof. Mohammad Al Faruque \u00b7 2024\u5e746\u6708 \u2013 2024\u5e7410\u6708"},{"location":"zh/#honors-and-awards","title":"\u8363\u8a89\u5956\u9879 (Honors and Awards)","text":"<ul> <li>\u82f1\u8bfa\u56fd\u9645\u4ea4\u6d41\u5956\u5b66\u91d1\uff0c\u6b66\u6c49\u5927\u5b66\uff0c2024</li> <li>\u82f1\u8bfa\u5353\u8d8a\u5956\u5b66\u91d1\uff08Top 3%\uff09\uff0c\u6b66\u6c49\u5927\u5b66\uff0c2023\uff0c2024</li> <li>\u5b66\u672f\u4f18\u79c0\u5956\u5b66\u91d1\uff08Top 5%\uff09\uff0c\u5f18\u6bc5\u5b66\u5802\uff0c2022\uff0c2023\uff0c2024</li> <li>\u7532\u7b49\u5956\u5b66\u91d1\uff08Top 5%\uff09\uff0c\u6b66\u6c49\u5927\u5b66\uff0c2022\uff0c2023\uff0c2024</li> <li>\u4e13\u5229: \u4e00\u79cd\u8282\u80fd\u8ba1\u7b97\u65b9\u6cd5, CN116085952.</li> </ul>"},{"location":"zh/#academic-services","title":"\u5b66\u672f\u670d\u52a1 (Academic Services)","text":"<ul> <li>\u4f1a\u8bae\u5ba1\u7a3f\u4eba: ICML\u201926, CVPR\u201926, ICLR\u201926, AAAI\u201926, ICASSP\u201926</li> <li>\u671f\u520a\u5ba1\u7a3f\u4eba: Neurocomputing, Frontiers in Oncology, IEEE Transactions on Networking, Frontiers in Medicine</li> </ul> <p>\u8fd9\u4e2a\u7f51\u7ad9\u6a21\u7248\u662f\u4ece\u6211\u6700\u597d\u7684\u670b\u53cb\u90a3\u91cc\u5077\u6765\u7684\u3002</p>"},{"location":"zh/blog/","title":"\u535a\u5ba2","text":"<p>\u8fd9\u91cc\u7528\u6765\u8bb0\u5f55\u6211\u7684\u7814\u7a76\u7b14\u8bb0\u3001\u590d\u73b0\u5b9e\u9a8c\u7684\u5de5\u7a0b\u7ecf\u9a8c\uff0c\u4ee5\u53ca\u4e00\u4e9b\u4e2a\u4eba\u601d\u8003\u3002</p>"},{"location":"zh/blog/#_2","title":"\u6587\u7ae0\u5217\u8868","text":"<ul> <li>\u656c\u8bf7\u671f\u5f85\u3002</li> </ul>"},{"location":"zh/blog/#_3","title":"\u4e3b\u9898","text":"<ul> <li>\u5f3a\u5316\u5b66\u4e60\u4e0e\u540e\u8bad\u7ec3</li> <li>\u751f\u6210\u5f0f AI</li> <li>\u79d1\u7814\u5de5\u7a0b\u5b9e\u8df5\uff08\u5b9e\u9a8c\u3001\u5de5\u5177\u94fe\u3001\u5199\u4f5c\uff09</li> </ul>"}]}